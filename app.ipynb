{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "Stock is really big nowadays. Everyone is doing it to get some money out of it, but there’s really no set equation to earn big money. However, what if you can predict the increase or decrease in stocks by analyzing articles? Afterall, what happens to big corporations tends to be on the news, and based on what happen to a corporation, the stock price changes. So why not analyze the news article and predict if stock price will increase toward our favor or not? In making this possible, we have this tool called Text Sentiment Analysis API offered by Google Cloud that we can use to analyze the sentiment of a document. What we’ll be doing is scrape a few articles from big publishers like Forbes about a given corporation and compare the ratio of positive and negative articles. If there’s more positive articles than negative articles, we’ll predict that the stock price for that corporation would go up and same thing for the other case. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: google-cloud-language in /Users/wonwoonam/opt/anaconda3/lib/python3.8/site-packages (2.0.0)\n",
      "Requirement already satisfied: libcst>=0.2.5 in /Users/wonwoonam/opt/anaconda3/lib/python3.8/site-packages (from google-cloud-language) (0.3.18)\n",
      "Requirement already satisfied: google-api-core[grpc]<2.0.0dev,>=1.22.2 in /Users/wonwoonam/opt/anaconda3/lib/python3.8/site-packages (from google-cloud-language) (1.26.3)\n",
      "Requirement already satisfied: proto-plus>=1.10.0 in /Users/wonwoonam/opt/anaconda3/lib/python3.8/site-packages (from google-cloud-language) (1.18.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.2 in /Users/wonwoonam/opt/anaconda3/lib/python3.8/site-packages (from libcst>=0.2.5->google-cloud-language) (3.7.4.3)\n",
      "Requirement already satisfied: pyyaml>=5.2 in /Users/wonwoonam/opt/anaconda3/lib/python3.8/site-packages (from libcst>=0.2.5->google-cloud-language) (5.3.1)\n",
      "Requirement already satisfied: typing-inspect>=0.4.0 in /Users/wonwoonam/opt/anaconda3/lib/python3.8/site-packages (from libcst>=0.2.5->google-cloud-language) (0.6.0)\n",
      "Requirement already satisfied: requests<3.0.0dev,>=2.18.0 in /Users/wonwoonam/opt/anaconda3/lib/python3.8/site-packages (from google-api-core[grpc]<2.0.0dev,>=1.22.2->google-cloud-language) (2.24.0)\n",
      "Requirement already satisfied: six>=1.13.0 in /Users/wonwoonam/opt/anaconda3/lib/python3.8/site-packages (from google-api-core[grpc]<2.0.0dev,>=1.22.2->google-cloud-language) (1.15.0)\n",
      "Requirement already satisfied: google-auth<2.0dev,>=1.21.1 in /Users/wonwoonam/opt/anaconda3/lib/python3.8/site-packages (from google-api-core[grpc]<2.0.0dev,>=1.22.2->google-cloud-language) (1.28.0)\n",
      "Requirement already satisfied: packaging>=14.3 in /Users/wonwoonam/opt/anaconda3/lib/python3.8/site-packages (from google-api-core[grpc]<2.0.0dev,>=1.22.2->google-cloud-language) (20.4)\n",
      "Requirement already satisfied: pytz in /Users/wonwoonam/opt/anaconda3/lib/python3.8/site-packages (from google-api-core[grpc]<2.0.0dev,>=1.22.2->google-cloud-language) (2020.1)\n",
      "Requirement already satisfied: setuptools>=40.3.0 in /Users/wonwoonam/opt/anaconda3/lib/python3.8/site-packages (from google-api-core[grpc]<2.0.0dev,>=1.22.2->google-cloud-language) (50.3.1.post20201107)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.6.0 in /Users/wonwoonam/opt/anaconda3/lib/python3.8/site-packages (from google-api-core[grpc]<2.0.0dev,>=1.22.2->google-cloud-language) (1.53.0)\n",
      "Requirement already satisfied: protobuf>=3.12.0 in /Users/wonwoonam/opt/anaconda3/lib/python3.8/site-packages (from google-api-core[grpc]<2.0.0dev,>=1.22.2->google-cloud-language) (3.15.7)\n",
      "Requirement already satisfied: grpcio<2.0dev,>=1.29.0; extra == \"grpc\" in /Users/wonwoonam/opt/anaconda3/lib/python3.8/site-packages (from google-api-core[grpc]<2.0.0dev,>=1.22.2->google-cloud-language) (1.37.0)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /Users/wonwoonam/opt/anaconda3/lib/python3.8/site-packages (from typing-inspect>=0.4.0->libcst>=0.2.5->google-cloud-language) (0.4.3)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /Users/wonwoonam/opt/anaconda3/lib/python3.8/site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core[grpc]<2.0.0dev,>=1.22.2->google-cloud-language) (1.25.11)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/wonwoonam/opt/anaconda3/lib/python3.8/site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core[grpc]<2.0.0dev,>=1.22.2->google-cloud-language) (2020.6.20)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /Users/wonwoonam/opt/anaconda3/lib/python3.8/site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core[grpc]<2.0.0dev,>=1.22.2->google-cloud-language) (3.0.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /Users/wonwoonam/opt/anaconda3/lib/python3.8/site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core[grpc]<2.0.0dev,>=1.22.2->google-cloud-language) (2.10)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3.6\" in /Users/wonwoonam/opt/anaconda3/lib/python3.8/site-packages (from google-auth<2.0dev,>=1.21.1->google-api-core[grpc]<2.0.0dev,>=1.22.2->google-cloud-language) (4.7.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /Users/wonwoonam/opt/anaconda3/lib/python3.8/site-packages (from google-auth<2.0dev,>=1.21.1->google-api-core[grpc]<2.0.0dev,>=1.22.2->google-cloud-language) (0.2.8)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /Users/wonwoonam/opt/anaconda3/lib/python3.8/site-packages (from google-auth<2.0dev,>=1.21.1->google-api-core[grpc]<2.0.0dev,>=1.22.2->google-cloud-language) (4.2.1)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /Users/wonwoonam/opt/anaconda3/lib/python3.8/site-packages (from packaging>=14.3->google-api-core[grpc]<2.0.0dev,>=1.22.2->google-cloud-language) (2.4.7)\n",
      "Requirement already satisfied: pyasn1>=0.1.3 in /Users/wonwoonam/opt/anaconda3/lib/python3.8/site-packages (from rsa<5,>=3.1.4; python_version >= \"3.6\"->google-auth<2.0dev,>=1.21.1->google-api-core[grpc]<2.0.0dev,>=1.22.2->google-cloud-language) (0.4.8)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install google-cloud-language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already up-to-date: google-cloud-language in /Users/wonwoonam/opt/anaconda3/lib/python3.8/site-packages (2.0.0)\n",
      "Requirement already satisfied, skipping upgrade: libcst>=0.2.5 in /Users/wonwoonam/opt/anaconda3/lib/python3.8/site-packages (from google-cloud-language) (0.3.18)\n",
      "Requirement already satisfied, skipping upgrade: proto-plus>=1.10.0 in /Users/wonwoonam/opt/anaconda3/lib/python3.8/site-packages (from google-cloud-language) (1.18.1)\n",
      "Requirement already satisfied, skipping upgrade: google-api-core[grpc]<2.0.0dev,>=1.22.2 in /Users/wonwoonam/opt/anaconda3/lib/python3.8/site-packages (from google-cloud-language) (1.26.3)\n",
      "Requirement already satisfied, skipping upgrade: pyyaml>=5.2 in /Users/wonwoonam/opt/anaconda3/lib/python3.8/site-packages (from libcst>=0.2.5->google-cloud-language) (5.3.1)\n",
      "Requirement already satisfied, skipping upgrade: typing-extensions>=3.7.4.2 in /Users/wonwoonam/opt/anaconda3/lib/python3.8/site-packages (from libcst>=0.2.5->google-cloud-language) (3.7.4.3)\n",
      "Requirement already satisfied, skipping upgrade: typing-inspect>=0.4.0 in /Users/wonwoonam/opt/anaconda3/lib/python3.8/site-packages (from libcst>=0.2.5->google-cloud-language) (0.6.0)\n",
      "Requirement already satisfied, skipping upgrade: protobuf>=3.12.0 in /Users/wonwoonam/opt/anaconda3/lib/python3.8/site-packages (from proto-plus>=1.10.0->google-cloud-language) (3.15.7)\n",
      "Requirement already satisfied, skipping upgrade: pytz in /Users/wonwoonam/opt/anaconda3/lib/python3.8/site-packages (from google-api-core[grpc]<2.0.0dev,>=1.22.2->google-cloud-language) (2020.1)\n",
      "Requirement already satisfied, skipping upgrade: six>=1.13.0 in /Users/wonwoonam/opt/anaconda3/lib/python3.8/site-packages (from google-api-core[grpc]<2.0.0dev,>=1.22.2->google-cloud-language) (1.15.0)\n",
      "Requirement already satisfied, skipping upgrade: packaging>=14.3 in /Users/wonwoonam/opt/anaconda3/lib/python3.8/site-packages (from google-api-core[grpc]<2.0.0dev,>=1.22.2->google-cloud-language) (20.4)\n",
      "Requirement already satisfied, skipping upgrade: googleapis-common-protos<2.0dev,>=1.6.0 in /Users/wonwoonam/opt/anaconda3/lib/python3.8/site-packages (from google-api-core[grpc]<2.0.0dev,>=1.22.2->google-cloud-language) (1.53.0)\n",
      "Requirement already satisfied, skipping upgrade: setuptools>=40.3.0 in /Users/wonwoonam/opt/anaconda3/lib/python3.8/site-packages (from google-api-core[grpc]<2.0.0dev,>=1.22.2->google-cloud-language) (50.3.1.post20201107)\n",
      "Requirement already satisfied, skipping upgrade: google-auth<2.0dev,>=1.21.1 in /Users/wonwoonam/opt/anaconda3/lib/python3.8/site-packages (from google-api-core[grpc]<2.0.0dev,>=1.22.2->google-cloud-language) (1.28.0)\n",
      "Requirement already satisfied, skipping upgrade: requests<3.0.0dev,>=2.18.0 in /Users/wonwoonam/opt/anaconda3/lib/python3.8/site-packages (from google-api-core[grpc]<2.0.0dev,>=1.22.2->google-cloud-language) (2.24.0)\n",
      "Requirement already satisfied, skipping upgrade: grpcio<2.0dev,>=1.29.0; extra == \"grpc\" in /Users/wonwoonam/opt/anaconda3/lib/python3.8/site-packages (from google-api-core[grpc]<2.0.0dev,>=1.22.2->google-cloud-language) (1.37.0)\n",
      "Requirement already satisfied, skipping upgrade: mypy-extensions>=0.3.0 in /Users/wonwoonam/opt/anaconda3/lib/python3.8/site-packages (from typing-inspect>=0.4.0->libcst>=0.2.5->google-cloud-language) (0.4.3)\n",
      "Requirement already satisfied, skipping upgrade: pyparsing>=2.0.2 in /Users/wonwoonam/opt/anaconda3/lib/python3.8/site-packages (from packaging>=14.3->google-api-core[grpc]<2.0.0dev,>=1.22.2->google-cloud-language) (2.4.7)\n",
      "Requirement already satisfied, skipping upgrade: rsa<5,>=3.1.4; python_version >= \"3.6\" in /Users/wonwoonam/opt/anaconda3/lib/python3.8/site-packages (from google-auth<2.0dev,>=1.21.1->google-api-core[grpc]<2.0.0dev,>=1.22.2->google-cloud-language) (4.7.2)\n",
      "Requirement already satisfied, skipping upgrade: cachetools<5.0,>=2.0.0 in /Users/wonwoonam/opt/anaconda3/lib/python3.8/site-packages (from google-auth<2.0dev,>=1.21.1->google-api-core[grpc]<2.0.0dev,>=1.22.2->google-cloud-language) (4.2.1)\n",
      "Requirement already satisfied, skipping upgrade: pyasn1-modules>=0.2.1 in /Users/wonwoonam/opt/anaconda3/lib/python3.8/site-packages (from google-auth<2.0dev,>=1.21.1->google-api-core[grpc]<2.0.0dev,>=1.22.2->google-cloud-language) (0.2.8)\n",
      "Requirement already satisfied, skipping upgrade: chardet<4,>=3.0.2 in /Users/wonwoonam/opt/anaconda3/lib/python3.8/site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core[grpc]<2.0.0dev,>=1.22.2->google-cloud-language) (3.0.4)\n",
      "Requirement already satisfied, skipping upgrade: idna<3,>=2.5 in /Users/wonwoonam/opt/anaconda3/lib/python3.8/site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core[grpc]<2.0.0dev,>=1.22.2->google-cloud-language) (2.10)\n",
      "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /Users/wonwoonam/opt/anaconda3/lib/python3.8/site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core[grpc]<2.0.0dev,>=1.22.2->google-cloud-language) (2020.6.20)\n",
      "Requirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /Users/wonwoonam/opt/anaconda3/lib/python3.8/site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core[grpc]<2.0.0dev,>=1.22.2->google-cloud-language) (1.25.11)\n",
      "Requirement already satisfied, skipping upgrade: pyasn1>=0.1.3 in /Users/wonwoonam/opt/anaconda3/lib/python3.8/site-packages (from rsa<5,>=3.1.4; python_version >= \"3.6\"->google-auth<2.0dev,>=1.21.1->google-api-core[grpc]<2.0.0dev,>=1.22.2->google-cloud-language) (0.4.8)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install --upgrade google-cloud-language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: MechanicalSoup in /Users/wonwoonam/opt/anaconda3/lib/python3.8/site-packages (1.0.0)\n",
      "Requirement already satisfied: beautifulsoup4>=4.4 in /Users/wonwoonam/opt/anaconda3/lib/python3.8/site-packages (from MechanicalSoup) (4.9.3)\n",
      "Requirement already satisfied: six>=1.4 in /Users/wonwoonam/opt/anaconda3/lib/python3.8/site-packages (from MechanicalSoup) (1.15.0)\n",
      "Requirement already satisfied: lxml in /Users/wonwoonam/opt/anaconda3/lib/python3.8/site-packages (from MechanicalSoup) (4.6.1)\n",
      "Requirement already satisfied: requests>=2.0 in /Users/wonwoonam/opt/anaconda3/lib/python3.8/site-packages (from MechanicalSoup) (2.24.0)\n",
      "Requirement already satisfied: soupsieve>1.2; python_version >= \"3.0\" in /Users/wonwoonam/opt/anaconda3/lib/python3.8/site-packages (from beautifulsoup4>=4.4->MechanicalSoup) (2.0.1)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /Users/wonwoonam/opt/anaconda3/lib/python3.8/site-packages (from requests>=2.0->MechanicalSoup) (3.0.4)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /Users/wonwoonam/opt/anaconda3/lib/python3.8/site-packages (from requests>=2.0->MechanicalSoup) (1.25.11)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /Users/wonwoonam/opt/anaconda3/lib/python3.8/site-packages (from requests>=2.0->MechanicalSoup) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/wonwoonam/opt/anaconda3/lib/python3.8/site-packages (from requests>=2.0->MechanicalSoup) (2020.6.20)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install MechanicalSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting up\n",
    "Now, in order to use the API, you must register yourself in Google Cloud and get the client key json file. Because this is too much of a work that involves billing information and other sensitive information, I went ahead and made a temporary account and provided you with the key json file. (Do not overuse this key)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import language_v1\n",
    "import matplotlib.pyplot as plt\n",
    "import mechanicalsoup\n",
    "import os\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Coding\n",
    "Let us now get into coding. You first have to install google-cloud language as well as upgrade it. Then, you also need to install MechanicalSoup. MechanicalSoup will be useful in the process of web scraping and google-cloud language will be useful in using the Sentiment Analysis API.\n",
    "\n",
    "Before moving onto the actual code, we’ll first setup the client to use the Sentiment Analysis API. As mentioned above, the json file with private key is provided. Make sure the file is on the same folder as this file (app.py)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "fileName = \"key.json\"\n",
    "scriptDir = os.path.dirname(os.path.realpath('__file__'))\n",
    "a = os.path.join(scriptDir, fileName)\n",
    "client = language_v1.LanguageServiceClient.from_service_account_json(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraping Data\n",
    "In this section, we’ll be scraping data and returning the dataset of sentiment scores given a keyword. To describe the general procedure, we first declare a StatefulBrowser using Mechanicalsoup to interact with HTML elements. Then, we find the class that corresponds to the articles that we want to extract from Forbes. This is pretty similar to what we did with homework on Yelp API so this shouldn’t be that hard to understand. (In this tutorial, we’re using MechanicalSoup, not BeautifulSoup, because MechanicalSoup lets us do so much more with less lines of code when it comes to web scraping)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Forbes Example\n",
    "brw1 = mechanicalsoup.StatefulBrowser()\n",
    "url1 = \"https://www.forbes.com/search/?q=%s\" % ('tesla')\n",
    "brw1.open(url1)\n",
    "links1 = brw1.page.find_all(class_=\"stream-item et-promoblock-removeable-item et-promoblock-star-item\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The only downside of this is that you have to go to each of the website to find the format in which the articles are arranged in. As you can see when running the above code, 'links' just returns the class names and it's not so pretty to look at the class names in such a long list, so we'll try converting these class names into actual texts. But first, I have a few other examples of opening up the websites and collecting class names for other publishers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#New York Times Example\n",
    "brw2 = mechanicalsoup.StatefulBrowser()\n",
    "url2 = \"https://www.nytimes.com/search?query=%s\" % ('tesla')\n",
    "brw2.open(url2)\n",
    "links2 = brw2.page.find_all(class_=\"css-1l4w6pd\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll take a look at how the classifications of the articles are converted into actual documents of the articles' text. Below is just another step to ensure we go as deep into the websites' hiearchy as where we should go to scrape the articles. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "href = links1[0].a.get('href')\n",
    "brw1.open(href)\n",
    "class_dict1 = {\"class\":\"article-body fs-article fs-responsive-text current-article\"}\n",
    "body = brw1.page.find('div', class_dict1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will go into the actual analysis of the text sentiment. Surprisingly, this can be done with one line of code. But before that, we have to finish our extraction of the article text. To do this, we have to make sure the body we extracted above is not 'None' and then combine all the paragraphs for each article to have it analyzed for the sentimentality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "if body:\n",
    "    txts = []\n",
    "    contents = body.find_all('p')\n",
    "    for ext in contents:\n",
    "        txts.append(ext.getText())\n",
    "        text = (\" \".join(txts))\n",
    "        document = language_v1.Document(content=text, type_=language_v1.Document.Type.PLAIN_TEXT)\n",
    "        sentiment = client.analyze_sentiment(request={'document': document}).document_sentiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So this is it for analyzing sentiment of an article and we'll go right ahead in making a function that takes in a keyword of the corporation and returns the list of sentiment scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forbesSentiment(keyword, num):\n",
    "    brw = mechanicalsoup.StatefulBrowser()\n",
    "    brw.open(\"https://www.forbes.com/search/?q=%s\" % (keyword))\n",
    "    links = brw.page.find_all(class_=\"stream-item et-promoblock-removeable-item et-promoblock-star-item\")\n",
    "    lst = []\n",
    "    limit = 0\n",
    "    for lk in links:\n",
    "        if limit == num:  \n",
    "            break\n",
    "        href = lk.a.get('href')\n",
    "        brw.open(href)\n",
    "        class_dict = {\"class\":\"article-body fs-article fs-responsive-text current-article\"}\n",
    "        body = brw.page.find('div', class_dict )\n",
    "        if not body:    \n",
    "            continue\n",
    "        txts = []\n",
    "        contents = body.find_all('p')\n",
    "        for ext in contents:\n",
    "            txts.append(ext.getText())\n",
    "        text = (\" \".join(txts))\n",
    "        document = language_v1.Document(content=text, type_=language_v1.Document.Type.PLAIN_TEXT)\n",
    "        sentiment = client.analyze_sentiment(request={'document': document}).document_sentiment\n",
    "        lst.append(sentiment.score)\n",
    "        limit += 1\n",
    "    brw.close()\n",
    "    return lst    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To compare how the sentiment scores are different for each publishers, I've integrated a function that finds a list of sentiment scores from New York Times articles. This way, we can compare if one publisher is more negatively driven when writing articles than others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nyTimesSentiment(keyword, num):\n",
    "    brw = mechanicalsoup.StatefulBrowser()\n",
    "    brw.open(\"https://www.nytimes.com/search?query=%s\" % (keyword))\n",
    "    links = brw.page.find_all(class_=\"css-1l4w6pd\")\n",
    "    base = 'https://www.nytimes.com'\n",
    "    lst = []\n",
    "    limit = 0\n",
    "    for lk in links:\n",
    "        if limit == num:  \n",
    "            break\n",
    "        href = lk.a.get('href')\n",
    "        full = base + href\n",
    "        brw.open(full)\n",
    "        body = brw.page.find('section', {\"name\":\"articleBody\"})\n",
    "        if not body:    \n",
    "            break\n",
    "        txts = []\n",
    "        contents = body.find_all('p')\n",
    "        for ext in contents:\n",
    "            txts.append(ext.getText())\n",
    "        text = (\" \".join(txts))\n",
    "        document = language_v1.Document(content=text, type_=language_v1.Document.Type.PLAIN_TEXT)\n",
    "\n",
    "        sentiment = client.analyze_sentiment(request={'document': document}).document_sentiment\n",
    "        lst.append(sentiment.score)\n",
    "        limit += 1\n",
    "    brw.close()\n",
    "    return lst    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forbes Sentiment Scores: \n",
      "[-0.10000000149011612, 0.0, 0.10000000149011612, 0.0, 0.0, -0.10000000149011612, -0.20000000298023224, 0.0, 0.0, 0.0]\n",
      "New York Times Sentiment Scores: \n",
      "[-0.10000000149011612, -0.4000000059604645, -0.20000000298023224, -0.6000000238418579, -0.10000000149011612, -0.30000001192092896, -0.30000001192092896, 0.0, -0.10000000149011612, 0.0]\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    print(\"Forbes Sentiment Scores: \")\n",
    "    print(forbesSentiment('tesla', 10))\n",
    "    print(\"New York Times Sentiment Scores: \")\n",
    "    print(nyTimesSentiment('tesla', 10))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an additional visualization, we can plot the data using plt.hist. Increasing the number of articles to analyze will greatly increase the number of runtime, but by looking at the plotted graph, we can see if the articles are focused in the negative scores or the positive scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "dta = forbesSentiment('tesla', 20)\n",
    "plt.hist(dta, bins = 5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "dta = nyTimesSentiment('tesla', 20)\n",
    "plt.hist(dta, bins = 5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although the amount of data is insufficient, we can see that New York Times has more negative inclination than Forbes on articles about 'Tesla'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparing Data\n",
    "Now that we have scraped data and got a sentiment analysis on those data, we'll compare our prediction with the actual stock data set. We will use API from alphavantage and compare the data from yesterday's stock price change with our prediction on 10 most recent articles. \n",
    "\n",
    "The connection here has been written about in various papers and is used in some startups. If you want to learn and read more about this topic, here are some of the links to the papers written on this topic: \n",
    "\n",
    "Paper1: https://arxiv.org/pdf/1812.04199.pdf\n",
    "\n",
    "Paper2: https://www.tandfonline.com/doi/full/10.1080/24751839.2021.1874252#:~:text=1.-,Introduction,with%20positive%20or%20negative%20sentiments\n",
    "\n",
    "\n",
    "Let us now dig into coding so that we know if this theory is valid!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io, time, json\n",
    "import requests\n",
    "from datetime import datetime, timedelta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the sake of consistency, we'll use Tesla as the key corporation of analyzation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "key = \"AIOG7263S8YMMVNI\"\n",
    "url = \"https://www.alphavantage.co/query?function=TIME_SERIES_DAILY&symbol=TSLA&outputsize=1&apikey=\"+key\n",
    "response = requests.get(url)\n",
    "jsonData = response.json()\n",
    "daily_data = jsonData['Time Series (Daily)']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next snippet of code, we're basically getting yesterday's date and comparing the change of stock market price for Tesla by looking at opening price and closing prices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "yesterday = datetime.now() - timedelta(1)\n",
    "temp = datetime.date(yesterday)\n",
    "wkday = temp.weekday()\n",
    "if wkday == 5:\n",
    "    yesterday = datetime.now() - timedelta(2)\n",
    "elif wkday == 6:\n",
    "    yesterday = datetime.now() - timedelta(3)\n",
    "\n",
    "yesterday = datetime.strftime(yesterday, '%Y-%m-%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "ystrday_data = daily_data[yesterday]\n",
    "open_val = ystrday_data['1. open']\n",
    "open_val = float(open_val)\n",
    "close_val = ystrday_data['4. close']\n",
    "close_val = float(close_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_change = \"\"\n",
    "if open_val > close_val:\n",
    "    stock_change = \"decreased\"\n",
    "elif open_val < close_val:\n",
    "    stock_change = \"increased\"\n",
    "else:\n",
    "    stock_change = \"no change\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below sorts positive sentiment scores from negative scores and predict that the stock price will decrease if there's more negative scores than positive scores as well as the other way around."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "company = 'tesla'\n",
    "stment_scores = forbesSentiment(company, 10)\n",
    "neg_count = 0\n",
    "pos_count = 0\n",
    "for scores in stment_scores:\n",
    "    if scores>0:\n",
    "        pos_count +=0\n",
    "    elif scores<0:\n",
    "        neg_count +=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "prdiction = \"\"\n",
    "if neg_count > pos_count:\n",
    "    prdiction = \"decreased\"\n",
    "elif pos_count > neg_count:\n",
    "    prdiction = \"increased\"\n",
    "else:\n",
    "    prdiction = \"no change\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we compare the prediction from the actual data in order to see if our prediction was correct or not. Really, to make this equation work, I would have to compare what my prediction was with the stock price for that day, but the Alphavantage API doesn't offer current date's stock data. Therefore, we'll just work with the most recent data set avilable. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrong Prediction\n"
     ]
    }
   ],
   "source": [
    "if stment_scores == prdiction:\n",
    "    result = \"Accurate Prediction\"\n",
    "else:\n",
    "    result = \"Wrong Prediction\"\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tutorial was for me to briefly explain how the stock market could be predicted with sentimental analysis of articles. As mentioned before, there are papers published about this topic and because I didn't want to follow exactly the procedures that they took in those papaers, this was a chance for me to prove the theory in my own very way. So feel free to give me some comments on what should've been done or what you liked about this tutorial. If you want to learn more about this topic and about the google clound api, refer to the link that I attached in above and to this link: https://cloud.google.com/natural-language/docs/reference/libraries"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
